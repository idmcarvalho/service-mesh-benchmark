---
apiVersion: v1
kind: Namespace
metadata:
  name: ml-benchmark
  labels:
    workload-type: ml-batch
---
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-training-job
  namespace: ml-benchmark
  labels:
    workload: ml-training
spec:
  parallelism: 2
  completions: 5
  template:
    metadata:
      labels:
        app: ml-training
        version: v1
    spec:
      restartPolicy: OnFailure
      # Pod-level security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: ml-worker
        # Use pre-built image with pinned dependencies
        # Build with: docker build -t ml-workload:v1.0.0 -f docker/ml-workload/Dockerfile docker/ml-workload/
        image: ml-workload:v1.0.0
        imagePullPolicy: IfNotPresent
        # Container-level security context
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
              - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
      volumes:
      - name: tmp
        emptyDir: {}
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: ml-inference-job
  namespace: ml-benchmark
  labels:
    workload: ml-inference
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: ml-inference
        spec:
          restartPolicy: OnFailure
          # Pod-level security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            runAsGroup: 1000
            fsGroup: 1000
            seccompProfile:
              type: RuntimeDefault
          containers:
          - name: inference
            # Use pre-built image with pinned dependencies
            # Build with: docker build -t ml-inference:v1.0.0 -f docker/ml-workload/Dockerfile.inference docker/ml-workload/
            image: ml-inference:v1.0.0
            imagePullPolicy: IfNotPresent
            # Container-level security context
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              runAsUser: 1000
              capabilities:
                drop:
                  - ALL
            volumeMounts:
            - name: tmp
              mountPath: /tmp
            resources:
              requests:
                cpu: 200m
                memory: 256Mi
              limits:
                cpu: 500m
                memory: 512Mi
          volumes:
          - name: tmp
            emptyDir: {}
