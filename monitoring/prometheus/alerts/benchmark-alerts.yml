groups:
  - name: benchmark_api_alerts
    interval: 30s
    rules:
      - alert: APIDown
        expr: up{job="benchmark-api"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Benchmark API is down"
          description: "The Benchmark API has been down for more than 2 minutes."

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      - alert: SlowResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow API response time"
          description: "95th percentile response time is {{ $value }}s on {{ $labels.instance }}"

  - name: database_alerts
    interval: 30s
    rules:
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL database is down"
          description: "PostgreSQL has been down for more than 2 minutes."

      - alert: DatabaseConnectionsHigh
        expr: pg_stat_database_numbackends > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High number of database connections"
          description: "Database has {{ $value }} connections"

      - alert: DatabaseDiskSpacelow
        expr: (node_filesystem_avail_bytes{mountpoint="/var/lib/postgresql"} / node_filesystem_size_bytes{mountpoint="/var/lib/postgresql"}) * 100 < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database disk space low"
          description: "Only {{ $value | humanize }}% disk space remaining"

  - name: kubernetes_alerts
    interval: 30s
    rules:
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"

      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes node not ready"
          description: "Node {{ $labels.node }} is not ready"

      - alert: PodMemoryHigh
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod memory usage high"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit"

  - name: service_mesh_alerts
    interval: 30s
    rules:
      - alert: ServiceMeshSidecarDown
        expr: up{job=~"istio-proxy|cilium-agent|linkerd-proxy|consul-sidecar"} == 0
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Service mesh sidecar is down"
          description: "Sidecar {{ $labels.job }} is down on {{ $labels.instance }}"

      - alert: HighServiceMeshLatency
        expr: histogram_quantile(0.95, rate(istio_request_duration_milliseconds_bucket[5m])) > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High service mesh latency"
          description: "95th percentile latency is {{ $value }}ms"
